{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project - Credit Card Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion&Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In the project, I have explored both supervised and unsupervised learning to detect fraud. I believe we need to be flexible and adjust what techiques to use depending on the datasets.\n",
    "\n",
    "If we encounter imbalanced datasets in the future like what I have in this project, then we can consider the resampling methods, including SMOTE, Random Under Sampling, and Random Over Sampling. While we are evaluating models, we can also set up different metrics to help us decide which model has the best performance.\n",
    "\n",
    "The result shows that both Decision Tree and Random Forest models are able to obtain 100% accuracy rate while we use SMOTE, but only 92% and 93% accuracy while we use Random Under Sampling.\n",
    "\n",
    "Both Decision Tree and Random Forest models are able to obtain 100% accuracy rate. The optimized logistic regression model had a better generalization performance on the testing set with reduced variance as compared to the other models. While the baseline logistic regression had over-fitted, the Na√Øve Bayes model was unable to achieve higher scores in the classification report and normalized confusion matrix, as compared to the optimized logistic regression model.\n",
    "\n",
    "The use of optimization for logistic regression had a significant impact on the results with the following two factors being considered:\n",
    "\n",
    "1. SMOTE to create new synthetic points in order to have a balanced dataset\n",
    "2. Grid search to select the best hyper-parameters to maximize model performance\n",
    "\n",
    "Nonetheless, the results were also attributed by the unique strength of logistic regression, where it is intended for binary (two-class) classification problems. It predicts the probability of an instance belonging to the default class, which can be derived as a binary output variable (ie. 0 or 1 classification). By optimizing the logistic regression model, it results in an even better model suited for classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. PCA dataset\n",
    "\n",
    "2. Gridsearch CV "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
